# B-Series: Algebraic Analysis of Numerical Methods - Part 2

**Author:** John C. Butcher

**Series:** Springer Series in Computational Mathematics, Volume 55

---

Chapter 1
Differential equations, numerical methods and
algebraic analysis




1.1 Introduction

Differential equations and numerical methods

Ordinary differential equations are at the heart of mathematical modelling. Typically
ordinary differential equation systems arise as initial value problems
                                       
                      y (x) = f x, y(x) ,     y(x0 ) = y0 âˆˆ RN .

or, if y does not depend directly on x,
                                      
                        y (x) = f y(x) ,         y(x0 ) = y0 âˆˆ RN .              (1.1 a)

The purpose of an equation like this is to describe the behaviour of a physical or
other system and, at the same time, to predict future values of the time-dependant
variable y(x), whose components represent quantities of scientiï¬c interest.
   It is often more convenient, in speciï¬c situations, to formulate (1.1 a) in different
styles. For example, the components of y(x) might represent differently named
variables, and the formulation should express this. In other situations the problem
being modelled might be more naturally represented using a system of second, or
higher, order differential equations. However, we will usually use (1.1 a) as a standard
form for a differential system.
   Given x > x0 , the ï¬‚ow of (1.1 a) is the solution to this initial value problem
evaluated at x. This is sometimes written as e(xâˆ’x0 ) f y0 , but our preference will be to
write it as ï¬‚ow xâˆ’x y0 , where the nature of f is taken for granted.
                    0
   The predictive power of differential equations is used throughout science, even
when solutions cannot be obtained analytically, and this underlines the need for
numerical methods. This usually means that we need to approximate ï¬‚ow h y0 to
obtain a usable value of y(x0 + h). This can be repeated computationally to obtain, in
turn, y(x0 + h), y(x0 + 2h), y(x0 + 3h), . . . .
   Although many methods for carrying out the approximation to the ï¬‚ow are known,
we will emphasize Rungeâ€“Kutta methods, because these consist of approximating

Â© Springer Nature Switzerland AG 2021                                                   1
J. C. Butcher, B-Series, Springer Series in Computational Mathematics 55,
https://doi.org/10.1007/978-3-030-70956-3_1
2                           1 Differential equations, numerical methods and algebraic analysis

the solution at x0 + nh, n = 1, 2, 3, . . . , step by step. As an example of these methods,
choose one of the famous methods of Runge [82] (Runge, 1895), where the mapping
R h y0 = y1 is deï¬ned by
                                                              
                             y1 = y0 + h f y0 + 12 h f (y0 ) .                      (1.1 b)


Accuracy of numerical approximations
Accuracy of numerical methods will be approached, in this volume, through a study
of the formal Taylor expansions of the solution, and of numerical approximations
to the solution. The ï¬‚avour of the questions that arise is both combinatorial and
algebraic, because of the common structure of many of the formal expansions.
   For the problem (1.1 a), we will need to compare the mappings ï¬‚ow h and, for a
particular Rungeâ€“Kutta method, the mapping RK h . This leads us to consider the
difference
                                ï¬‚ow h y0 âˆ’ RK h y0 .
If it were possible to expand this expression in a Taylor series, then it would be
possible to seek methods for which the terms are zero up to some required power of
h, say to terms like h p . It would then be possible to estimate the asymptotic accuracy
of the error as O(h p+1 ). This would be only for a single step but this theory, if it
were feasible, would also give a guide to the global accuracy.


Taylor expansions and trees

Remarkably, ï¬‚ow h y0 and RK h y0 have closely related Taylor expansions, and one
of the ï¬rst aims of this book is to enunciate and analyse these expansions. The ï¬rst
step, in this formulation, is to make use of the graphs known as rooted trees, or
arborescences, but referred to throughout this book simply as trees.
   The formal introduction of trees will take place in Chapter 2 but, in the meantime,
we will introduce these objects by illustrative diagrams:

             ,       ,       ,       ,          ,         ,      ,       ,     ...

The set of all trees will be denoted by T.
    If t denotes an arbitrary tree, then |t| is the â€œorderâ€, or number of vertices, and Ïƒ (t)
is the symmetry of t. The symmetry is a positive integer indicating how repetitive a
tree diagram is. The formal statement will be given in Deï¬nition 2.5A (p. 58).
    The common form for ï¬‚ow h y0 and RK h y0 is

                                 y0 + âˆ‘ Ï‡(t) Ïƒ 1(t) F(t)h|t| ,                        (1.1 c)
                                         t

where, for a given tree, F(t) depends only on the differential equation being solved
and Ï‡(t) depends only on the mapping, ï¬‚ow h or RK h .
1.1 Introduction                                                                        3

   The formulation of various Taylor expansions, given by (1.1 c), is the essential
idea behind the theory of B-series, and is the central motivation for this book. We
will illustrate the use of this result, using three numerical methods from the present
chapter, together with the ï¬‚ow itself. The methods are the Euler method, Euler h ,
(1.4 a), the Rungeâ€“Kutta method, Runge-I h :
                                                                      
                        y1 = y0 + 12 h f (y0 ) + 12 h f y0 + h f (y0 )

and Runge-II h , given by (1.1 b). Alternative formulations of these Rungeâ€“Kutta
methods in Section 1.5 (p. 19) are, for Runge-I h , (1.5 c) and, for Runge-II h , (1.5 d).
  The coefï¬cients, that is the values of Î¨ (t), for |t| â‰¤ 3, are

                   Mapping             Î¨( ) Î¨( ) Î¨( ) Î¨( )
                                                    1         1          1
                   ï¬‚ow h                1           2         3          6
                   Euler h              1           0         0          0
                                                    1         1
                   Runge-I h            1           2         2          0
                                                    1         1
                   Runge-II h           1           2         4          0
    Independently of the choice of the differential equation system being solved, we
can now state the orders of the three methods under consideration. Because the same
entry is given for the single ï¬rst order tree, each of the three numerical methods is
at least ï¬rst order as an approximation to the exact solution. Furthermore, the two
Runge methods are order two but not three, as we see from the agreement with the
ï¬‚ow for the order 2 tree, but not for the two order 3 trees. Also, from the table entries,
we see that the Euler method does not have an order greater than one.


FreÌchet derivatives and gradients
In the formulation and analysis of both initial value problems, and numerical methods
for solving them, it will be necessary to introduce various structures involving partial
derivatives. In particular, the ï¬rst FreÌchet derivative, also known as the Jacobian
matrix, with elements
                                 â¡                             â¤
                                   âˆ‚ f1     âˆ‚ f1         âˆ‚ f1
                                                   Â· Â· Â· âˆ‚ yN
                                 â¢ âˆ‚y 1     âˆ‚y   2             â¥
                                 â¢ âˆ‚ f2     âˆ‚ f2         âˆ‚ f2 â¥
                                 â¢                 Â· Â· Â· âˆ‚ yN â¥
                                 â¢                             â¥
                       f  (y) = â¢ âˆ‚ y      âˆ‚ y2
                                      1
                                                                 .
                                 â¢ ..         ..           .. â¥â¥
                                 â¢ .           .            . â¥
                                 â£                             â¦
                                   âˆ‚ fN    âˆ‚ fN          âˆ‚ fN
                                                   Â· Â· Â· âˆ‚ yN
                                      1âˆ‚y      âˆ‚y2


Similarly the FreÌchet derivative of a scalar-valued function has the form

                           H  (y) =    âˆ‚H
                                        âˆ‚ y1
                                               âˆ‚H
                                               âˆ‚ y2
                                                        Â·Â·Â·   âˆ‚H
                                                              âˆ‚ yN
                                                                     .
4                             1 Differential equations, numerical methods and algebraic analysis

This is closely related to the gradient âˆ‡(H) = H  (y)T which arises in many speciï¬c
problems and classes of problems.


Chapter outline

In Section 1.2, a review of differential equations is presented. This is followed
in Section 1.3 by examples of differential equations, The Euler and Taylor series
methods are introduced in Section 1.4 followed by Rungeâ€“Kutta methods (Section
1.5), and multivalue methods (Section 1.6). Finally, a preliminary introduction to
B-series is presented in Section 1.7.


1.2 Differential equations

An ordinary differential equation is expressed in the form
                      dy            
                      d x = f x, y(x) ,            f : R Ã— R N â†’ RN                     (1.2 a)

or, written in terms of individual components,
                        d y1      1                          
                        d x = f x, y (x), y (x), . . . , y (x) ,
                               1           2              N

                        d y2      1                          
                        d x = f x, y (x), y (x), . . . , y (x) ,
                               2           2              N

                         ..          ..                                                 (1.2 b)
                          .           .
                       d yN       1                          
                        d x = f x, y (x), y (x), . . . , y (x) .
                               N           2              N


This can be formulated as an autonomous problem
                              dy         
                              d x = f y(x) ,       f : R N â†’ RN ,                       (1.2 c)

by increasing N if necessary and introducing a new dependent variable y0 which is
forced to always equal x. This autonomous form of (1.2 b) becomes

                     d y0
                           = 1,
                      dx
                     d y1                                             
                           = f 1 y0 (x), y1 (x), y2 (x), . . . , yN (x) ,
                      dx
                     d y2                                             
                           = f 2 y0 (x), y1 (x), y2 (x), . . . , yN (x) ,
                      dx
                      ..        ..
                       .         .
                     dy  N                                            
                           = f N y0 (x), y1 (x), y2 (x), . . . , yN (x) .
                     dx
1.2 Differential equations                                                                           5

Initial value problems

A subsidiary condition

                              y(x0 ) = y0 ,        x0 âˆˆ R,        y 0 âˆˆ RN ,                    (1.2 d)

is an initial value and an initial value problem consists of the pair of equations (1.2 a),
(1.2 d) or the pair (1.2 c), (1.2 d).
    Initial value problems have applications in applied mathematics, engineering,
physics and other sciences, and ï¬nding reliable and efï¬cient numerical methods for
their solution is of vital importance.

Exercise 1 Reformulate the initial value problem

                    u (x) + 3u (x) = 2u(x) + v(x) + cos(x),     u(1) = 2,   u (1) = âˆ’2,
             v (x) + u (x) âˆ’ v (x) = u(x) + v(x)2 + sin(x),    v(1) = 1,   v (1) = 4,
                          
in the form y (x) = f y(x) , y(x0 ) = y0 , where y0 = x, y1 = u, y2 = u , y3 = v, y4 = v .


Scalar problems
If N = 1, we obtain a scalar initial value problem
                                        
                      y (x) = f x, y(x) ,      y(x0 ) = y0 âˆˆ R.                                (1.2 e)

Scalar problems are useful models for more general problems, because of their sim-
plicity and ease of analysis. However, this simplicity can lead to spurious conclusions.
A speciï¬c case is the early analysis of Rungeâ€“Kutta order conditions [82] (Runge,
1895), [56] (Heun, 1900), [66] (Kutta, 1901), [77] NystroÌˆm, 1925), in which, above
order 4, the order conditions derived using (1.2 e) give an incomplete set.

Complex variables
Sometimes it is convenient to write a differential equation using complex variables
                           dz           
                              = f t, z(t) ,            f : R Ã— C N â†’ CN .
                           dt
For example, the system
                                 dx
                                    = 2x + 3 cos(t),         x(0) = 1,
                                 dt
                                 dy
                                    = 2y + sin(t),           y(0) = 0,
                                 dt
can be written succinctly as
                         dz
                            = 2z + 2 exp(it) + exp(âˆ’it),              z(0) = 1,                 (1.2 f)
                         dt
with z(t) = x(t) + iy(t).
6                              1 Differential equations, numerical methods and algebraic analysis

Exercise 2 Find the values of A, B, C such that z = A exp(2t) + B exp(it) +C exp(âˆ’it) is the
solution to (1.2 f).

Exercise 3 Write the solution to Exercise 2 in terms of the real and imaginary components.




Well-posed problems

An initial value problem is well-posed if it has a solution, this solution is unique
and the solution depends continuously on the initial value. In this discussion we will
conï¬ne ourselves to autonomous problems.

    Deï¬nition 1.2A A function f : RN â†’ RN satisï¬es a Lipschitz condition if there
    exists a constant L > 0 (the Lipschitz constant) such that

                          f (y) âˆ’ f (z) â‰¤ Ly âˆ’ z,                y, z âˆˆ RN .

Given an initial value problem
                                            
                              y (x) = f y(x) ,             y(x0 ) = y0 ,

where f satisï¬es a Lipschitz condition with constant L, we ï¬nd by integration that
for x â‰¥ x0 ,
                                         x      
                           y(x) = y0 +     f y(x) d x.                     (1.2 g)
                                                   x0

If x âˆˆ I := [x0 , x], and y denotes supxâˆˆI y(x), we can construct a sequence of
approximations y[k] , k = 0, 1, . . . , to (1.2 g), from

                    y[0] (x) = y0 ,
                                         x              
                      y[k] (x) = y0 +        f y[kâˆ’1] (x) d x,        k = 1, 2, . . . .
                                        x0

If r := |x âˆ’ x0 | L < 1, we obtain the estimates

                    y[1] âˆ’ y[0]  â‰¤ |x âˆ’ x0 |  f (y0 ),                                (1.2 h)
                 y [k+1]
                            âˆ’ y  â‰¤ ry âˆ’ y
                              [k]            [k]   [kâˆ’1]
                                                            â‰¤ r |x âˆ’ x0 |  f (y0 ).
                                                                 k
                                                                                          (1.2 i)

This shows that the sequence y[k] , k = 0, 1, . . . , is convergent. Denote the limit by y.
It can be veriï¬ed that the conditions for well-posedness are satisï¬ed.
    By adding (1.2 h) and (1.2 i), with k = 1, 2, . . . , we see that every member of the
sequence satisï¬es
                                           1
                         y[k] âˆ’ y[0]  â‰¤      |x âˆ’ x0 |  f (y0 ).
                                          1âˆ’r
1.2 Differential equations                                                           7

To overcome the restriction |x âˆ’ x0 | L < 1, a sequence of x values can be inserted
between x0 and x, sufï¬ciently close together to obtain convergent sequences in each
subinterval in turn.
   While a Lipschitz condition is very convenient to use in applications, it is not
a realistic assumption, because many well-posed problems do not satisfy it. It is
perhaps better to use the property given in the following.


 Deï¬nition 1.2B A function f : RN â†’ RN satisï¬es a local Lipschitz condition
 if there exists a constant L (the Lipschitz constant) and a positive real R (the
 inï¬‚uence radius) such that

               f (y) âˆ’ f (z) â‰¤ Ly âˆ’ z,           y, z âˆˆ RN ,     y âˆ’ z â‰¤ 2R.

If f satisï¬es the conditions of Deï¬nition 1.2B, then for a a given y0 âˆˆ RN , deï¬ne a
disc D by
                            D = {y âˆˆ RN : y âˆ’ y0  â‰¤ R}

and a function f by
                               â§
                               âª
                               â¨              f (y),                 y âˆˆ D,
                     f(y) =
                               â© f y + R (y âˆ’ y ),
                               âª
                                                                     y âˆˆ D.
                                     0 yâˆ’y    0
                                                 0




Exercise 4 Show that f satisï¬es a Lipschitz condition with Lipschitz constant L.




The ï¬rst numerical methods

The method of Euler [42] (Euler, Collected works, 1913), proposed in the eighteenth
century, is regarded as the foundation of numerical time-stepping methods for the
solution of differential equations. We will refer to it here as the â€œexplicit Eulerâ€
method to distinguish it from the closely related â€œimplicit Eulerâ€ method. Given a
problem                                   
                         y (x) = f x, y(x) ,   y(x0 ) = y0 ,
we can try to approximate the solution at a nearby point x1 = x0 + h, by the formula

                               y(x1 ) â‰ˆ y1 := y0 + h f (x0 , y0 ).

  This is illustrated in the one-dimensional case by the diagram on the left (Explicit
Euler).
8                            1 Differential equations, numerical methods and algebraic analysis

         Explicit Euler                                    Implicit Euler
                                                                            y(x)
                          y(x)

                                                                                    y1
                                  y1
                                                               y0
             y0
                     h                                                 h
           x0                    x1                           x0                   x1

    According to this diagram, y1 âˆ’ y0 is calculated as the area of the rectangle with
width h and height f (x0 , y0 ). This is not the correct answer, for which h should be
multiplied by the average value of f (y(x)), but it is often close enough to give useful
results for small h. In the diagram on the right (Implicit Euler), the value of y1 âˆ’ y0
is h is multiplied by f (y1 ), which is not known explicitly but can be evaluated by
iteration in the formula
                                  y1 = y0 + h f (x1 , y1 ).
We will return to the Euler method in Section 1.4.



1.3 Examples of differential equations

Linear problems

Exponential growth and decay
                                    dy
                                        = Î» y.
                                    dx
If Î» > 0, the solution represents exponential growth and, if Î» < 0, the solution
represents exponential decay. Two cases can be combined into a single system
                                               
                                d y1           y1
                                        =           .
                               d x y2       âˆ’y2

This can also be written
                                                     
                                               0 1
                                      y =                 âˆ‡(y1 y2 )
                                             âˆ’1 0
and is an example of a Poisson problem

                                          y (x) = Sâˆ‡(H(y)),                             (1.3 a)

where S is a skew-symmetric matrix. For such problems H(y(x)) has a constant value,
because                         
                       d H y(x)        âˆ‚ H   âˆ‚ H T
                                  =         S         = 0.
                           dx           âˆ‚y      âˆ‚y
1.3 Examples of differential equations                                                9

It is an important aim in numerical analysis to preserve this invariance property, in
computational results.


A four-dimensional linear problem
The problem                                â¡          â¤
                                            âˆ’2 1 0 0
                                           â¢          â¥
                                           â¢ 1 âˆ’2 1 0 â¥
                         y = My,        M=â¢          â¥
                                           â¢ 0 1 âˆ’2 1 â¥ ,
                                           â£          â¦
                                             0 0 1 âˆ’2
is a trivial special case of the discretized diffusion equation on an interval domain. A
transformation M â†’ M = T âˆ’1 MT , where
                      â¡                  â¤            â¡                 â¤
                         1 0 0 1                        âˆ’2 1 0 0
                      â¢                  â¥            â¢                 â¥
                      â¢ 0 1 1 0 â¥                     â¢ 1 âˆ’1 0 0 â¥
                T =â¢  â¢ 0 1 âˆ’1 0 â¥
                                         â¥,      M  = â¢
                                                      â¢ 0 0 âˆ’3 1 â¥ ,
                                                                        â¥
                      â£                  â¦            â£                 â¦
                         1 0 0 âˆ’1                        0 0 1 âˆ’2

partitions the problem into symmetric and anti-symmetric components. Also write
y = T âˆ’1 y, y0 = T âˆ’1 y0 so that the partitioned initial value problem becomes

                                 y = M y,     y(x0 ) = y0 .

Making this transformation converts the problem into two separate two dimensional
problems which can be solved independently and the results recombined.


Harmonic oscillator and simple pendulun
The harmonic oscillator:                      
                                     d y1     y2
                                           =       .
                                    d x y2   âˆ’y1
This equation can be recast in scalar complex form by introducing a new variable
z = y1 + iy2 . It then becomes
                                     dz
                                        = âˆ’iz.
                                    dx
The harmonic oscillator can also be written in the form (1.3 a), with
                                                        
                                H(y) = 12 (y1 )2 + (y2 )2 .

The simple pendulum:                              
                                  d y1        y2
                                        =              .
                                 d x y2   âˆ’ sin(y1 )
10                             1 Differential equations, numerical methods and algebraic analysis

This problem is not linear but, if y(0) is sufï¬ciently small, the simple pendulum is
a reasonable approximation to a linear problem, because sin(y1 ) â‰ˆ y1 . It also has the
form of (1.3 a) with H(y) = 12 (y2 )2 âˆ’ cos(y1 ).


Stiff problems

Many problems arising in scientiï¬c modelling have a special property known as
â€œstiffnessâ€, which makes numerical solution by classical methods very difï¬cult. An
early reference is [35] (Curtiss, Hirschfelder,1952). For a contemporary study of
stiff problems, and numerical methods for their solution, see [53] (Hairer, NÃ¸rsett,
Wanner,1993) and [86] (SoÌˆderlind, Jay, Calvo, 2015).
    When attempting to determine the most appropriate stepsize to use with a par-
ticular method, and a particular problem, many considerations come into play. The
ï¬rst is the requirement that the truncation error is sufï¬ciently small to match the
requirements of the physical application, and the second is that the numerical results
are not corrupted unduly by unstable behaviour.
    To illustrate this idea, consider the use of the Euler method (see Section 1.4
(p. 14)), applied to the three-dimensional problem
        â¡     â¤ â¡                         â¤ â¡             â¤ â¡           â¤
           y1         âˆ’y2 + 0.40001(y3 )2          y1 (0)         0.998
     d â¢      â¥ â¢
        â¢ y2 â¥ = â¢
                                          â¥ â¢             â¥ â¢           â¥
                                          â¥ , â¢ y2 (0) â¥ = â¢ 0.00001 â¥ , (1.3 b)
                               y1
     dx â£     â¦ â£                         â¦ â£             â¦ â£           â¦
           y3                âˆ’100y3                y3 (0)           1

with exact solution
                 â¡             â¤    â¡                                      â¤
                      y1 (x)             cos(x) âˆ’ 0.002 exp(âˆ’200x)
                 â¢        â¥ â¢                               â¥
                 â¢ y2 (x) â¥ = â¢ sin(x) + 0.00001 exp(âˆ’200x) â¥ .
                 â£        â¦ â£                               â¦
                   y3 (x)                exp(âˆ’100x)

A solution by the Euler method consists of computing approximations

            y1 â‰ˆ y(x0 + h),        y2 â‰ˆ y(x0 + 2h),      y3 â‰ˆ y(x0 + 3h),      ...,

using yn = F(ynâˆ’1 ), n = 1, 2, . . . , where
                            â¡                                â¤
                                 u1 + h âˆ’ u2 + 0.40001(u3 )2
                            â¢                                  â¥
                   F(u) = â¢ â£                u2 + hu1          â¥.
                                                               â¦
                                           (1 âˆ’ 100h)u3


   For sequences like this, stability, for the third component, depends on the condition
1 âˆ’ 100h â‰¥ âˆ’1 being satisï¬ed, so that h â‰¤ 0.02. If this condition is not satisï¬ed,
unstable behaviour of y3 will feed into the ï¬rst two components and the computed
1.3 Examples of differential equations                                                                      11

results cannot be relied on. However, if the initial value for y3 were zero, and this
component never drifted from this value, there would be no such restriction on
obtaining reliable answers.

Exercise 5 If problem (1.3 b) is solved using the implicit Euler method (1.4 d), ï¬nd the function F
such that yn = F(ynâˆ’1 ), and show that there is no restriction on positive h to yield stable results.




Test problems

A historical problem

The following one-dimensional non-autonomous problem was used by Runge and
others to verify the behaviour of some early Rungeâ€“Kutta methods:

                           dy yâˆ’x
                               =       ,     y(0) = 1.                        (1.3 c)
                           dx y+x
                                                   
A parametric solution t â†’ y(t), x(t) := y1 (t), y2 (t) can be found from the system
                                                                                               
              d        y1                   1 âˆ’1              y1               y1 (0)               1
                                    =                                  ,                    =
              dt       y2                   1   1             y2               y2 (0)               0

and, by writing z = y1 + iy2 , we obtain

                           dz
                               = (1 + i)z,       z(0) = 1,
                           dt
                             
with solution z = exp (1 + i)t , so that, reverting to the original notation,

                                                y(t) = exp(t) cos(t),
                                                x(t) = exp(t) sin(t).
                                                     
The solution on 0, exp( 12 Ï€) corresponds to t âˆˆ 0, 12 Ï€ and is shown in the diagram

                                                    t = Ï€/4
                       y

                            1 t =0


                                                                                  t = Ï€/2
                            0
                                0                         x                exp(Ï€/2)
12                        1 Differential equations, numerical methods and algebraic analysis

A problem from DETEST

One of the pioneering developments in the history of numerical methods for differ-
ential equations is the use of standardized test problems. These have been useful in
identifying reliable and accurate software. This problem from the DETEST set [57]
(Hull, Enright, Fellen, Sedgwick, 1972) is an interesting example.

                             dy
                                = cos(x)y,      y(0) = 1.
                             dx
                                          
The exact solution, given by y = exp sin(x) , is shown in the diagram

      exp(1)


      y

          1

     exp(âˆ’1)

                           Ï€/2                 Ï€                3Ï€/2                 2Ï€
                                                                            x



The Protheroâ€“Robinson problem

The problem of Prothero and Robinson [79] (1974),
                                                       
                            y (x) = g (x) + L y âˆ’ g(x) ,

where g(x) is a known function, was introduced as a model for studying the behaviour
of numerical methods applied to stiff problems. A special case is
                                              
                    y = cos(x) âˆ’ 10 y âˆ’ sin(x) ,         y(0) = 0,

with general solution y(x) = C exp(âˆ’10x) + sin(x), where C = 0 when y(0) = 0.



A problem with discontinuous derivatives

The two-dimensional â€œdiamond problemâ€, as we will name it, is deï¬ned to have
piecewise constant derivative values which change from quadrant to quadrant as
follows
1.3 Examples of differential equations                                                13
                             â§    
                             âª
                             âª âˆ’1
                             âª
                             âª       ,             y1 > 0,   y2 â‰¥ 0,
                             âª
                             âª
                             âª
                             âª   1
                             âª
                             âª    
                             âª
                             âª
                             âª
                             âª  âˆ’1
                             âª
                             âª       ,             y1 â‰¤ 0,   y2 > 0,
                        d y â¨ âˆ’1
                           =      
                        dx âª âª
                             âª
                             âª   1
                             âª
                             âª       ,             y1 < 0,   y2 â‰¤ 0,
                             âª
                             âª  âˆ’1
                             âª
                             âª    
                             âª
                             âª
                             âª
                             âª   1
                             âª
                             âª       ,             y1 â‰¥ 0,   y2 < 0.
                             â©
                                 1

   Using the initial value y = [ 1 0 ]T , the orbit, with period 4, is as in the diagram:

                                              1


                                               0         1




   This problem is interesting as a numerical test because of the non-smoothness of
the orbit as it moves from one quadrant to the next.


The Kepler problem
                                       â¡ â¤ â¡ 3â¤
                                         y1      y
                                       â¢ â¥ â¢  â¢
                                                    â¥
                                                   4â¥
                                       â¢    â¥
                                    d â¢y â¥ â¢ 1 â¥
                                          2   â¢  y
                                       â¢ â¥=      y â¥,                            (1.3 d)
                                   d x â¢y3 â¥ â¢  âˆ’ 3â¥
                                       â£ â¦ â£ r â¥
                                              â¢
                                                    â¦
                                                 y2
                                         y4     âˆ’ 3
                                                     r
                         1/2
where r = (y1 )2 + (y2 )2      . The Kepler problem satisï¬es conservation of energy
H  = 0, where                                     
                           H(x) = 12 (y3 )2 + (y4 )2 âˆ’ râˆ’1
and also conservation of angular momentum A = 0, where

                                    A(x) = y1 y4 âˆ’ y2 y3 .


Exercise 6 Show that H(x) is invariant.


Exercise 7 Show that A(x) is invariant.
14                               1 Differential equations, numerical methods and algebraic analysis

1.4 The Euler method

The explicit Euler method as a Taylor series method

Given a differential equation and an initial value,

                                 y (x) = f (x, y),        y(x0 ) = y0 ,

the Taylor series formula is a possible approach to ï¬nding an approximation to
y(x0 + h):
            y(x0 + h) â‰ˆ y(x0 ) + hy (x0 ) + 2!
                                             1 2                  1 p (p)
                                                h y (x0 ) + Â· Â· Â· + p! h y (x0 ).

If y is a sufï¬ciently smooth function, then we would expect the error in this ap-
proximation to be O(h p+1 ). When p = 1, this reduces to the Euler method. This is
very convenient to use, because both y(x0 ) = y0 and y (x0 ) = f (x0 , y0 ) are known
in advance. However, for p = 2, we would need the value of y (x0 ), which can be
found from the chain rule:
                            d           âˆ‚ f âˆ‚ f dy
                  y (x) = dx f x, y(x) = âˆ‚ x + âˆ‚ y dx = fx + fy f ,

where the subscripts in fx and fy denote partial derivatives, and, for brevity, the
arguments have been suppressed. Restoring the arguments, we can write

                         y (x0 ) = fx (x0 , y0 ) + fy (x0 , y0 ) f (x0 , y0 ).

The increasingly more complicated expressions for y(3) , y(4) , . . . , have been worked
out at least to order 6 [59] (Hutâ€™a, 1956), and they are summarized here to order 4.

                  y = f ,
                 y = fx + fy f ,
                y(3) = fxx + 2 fxy f + fyy f 2 + fx fy + fy2 f ,
                y(4) = fxxx + 3 fxxy f + 3 fxy fx + 5 fxy fy f + 3 fxyy f 2 + fy fxx
                             + 3 fx fyy f + fy2 fx + fy3 f + 4 fy f 2 fyy + f 3 fyyy .

We will return to the evaluation of higher derivatives, in the case of an autonomous
system, in Section 1.7 (p. 33).

Exercise 8 Given the differential equation y = y + sin(x), ï¬nd y(n) for n â‰¤ 7.


The explicit Euler method

The Euler method produces the result

                      yk = ykâˆ’1 + h f (xkâˆ’1 , ykâˆ’1 ),             k = 1, 2, . . . .        (1.4 a)
1.4 The Euler method                                                                    15

In this introduction, it will be assumed that h is constant. Now consider a numerical
method of the form
                               yk = ykâˆ’1 + hÎ¨(xkâˆ’1 , ykâˆ’1 ),                   (1.4 b)
used in the same way as the Euler method.

 Deï¬nition 1.4A The method deï¬ned by (1.4 b) is convergent if, for a problem
 deï¬ned by f (x, y), y(x0 ) = y0 , with the solution Yn , at x, approximated using n
 steps with h = (x âˆ’ x0 )/n, then

                                        lim Yn = y(x).
                                       nâ†’âˆ



 Theorem 1.4B The Euler method is convergent.

This result from [36] (Dahlquist, 1956), with an exposition in the classic textbook
[55] (Henrici, 1962), is also presented in the more recent books [50] (Hairer, NÃ¸rsett,
Wanner, 1993) and [20] (Butcher, 2016).


Variable stepsize
The standard formulation of a one-step method is based on a single input y0 , and its
purpose is to calculate a single output y1 . However, it is also possible to consider the
input as being the pair [y0 , h f0 ], with f0 = f (y0 ). In this case the output would be a
pair [y1 , h f1 ]. Apart from the inconvenience of passing additional data between steps,
the two formulations are identical.
   However, the two input approach has an advantage if the Euler method is required
to be executed as a variable stepsize method, as in the Octave function (1.4 c).As
we will see in Section 1.5, the Rungeâ€“Kutta method (1.5 c) has order 2. This would
mean that half the difference between the result computed by Euler, and the result
computed by this particular Rungeâ€“Kutta method, could be used as an error estimator
for the Euler result because y0 + 12 h f0 + 12 h f1 is identical to the result computed
by (1.5 c). This is the basis for the function represented in (1.4 c). Note that this
estimation does not require additional f calculations.
   function [yout,hfyout,hout] = Euler(y,hfy,tolerance)
      yout = y + hfy;
      hfyout = h * f(yout);
      error = 0.5 * norm(hfy - hfyout);
      r = sqrt(tolerance / error);                                                 (1.4 c)
      hout = r * h;
      hfyout = r * hfyout;
   endfunction


Exercise 9 Discuss the imperfections in (1.4 c).
16                            1 Differential equations, numerical methods and algebraic analysis

The implicit Euler method
As we saw in Section 1.3, through the problem (1.3 b), there are sometimes advan-
tages in using the implicit version of (1.4 a), in the form

                          yk = ykâˆ’1 + h f (xk , yk ),      k = 1, 2, . . .                 (1.4 d)

This method also reappears as an example of the implicit theta Rungeâ€“Kutta method
(1.5 g) with Î¸ = 1.
     In the calculation of yk in (1.4 d), we need to solve an algebraic equation

                             Y âˆ’ h f (Y ) = C, where C = ykâˆ’1 .

If f satisï¬es a Lipschitz condition with |h|L < 1, then it is possible to use functional
iteration. That is, Y can be found numerically from the sequence Y [0] ,Y [1] ,Y [2] , . . . ,
where
                        Y [0] = C,
                          Y [n] = C + h f (Y [nâˆ’1] ),     n = 1, 2, . . .

To obtain rapid convergence, this simple iterative system can be replaced by the
quadratically-convergent Newton scheme:

     Y [0] = C,
                                           âˆ’1  [nâˆ’1]                    
     Y [n] = Y [nâˆ’1] âˆ’ I âˆ’ h f  (Y [nâˆ’1] )      Y      âˆ’C âˆ’ h f (Y [nâˆ’1] ) ,   n = 1, 2, . . .



Experiments with the explicit Euler method

The Kepler problem
The Kepler problem (1.3 d), with initial value y0 = [1, 0, 0, 1]T , has a circular orbit
solution with period 2Ï€. To see how well the Euler method is able to solve this
problem over a single orbit, a constant stepsize h = 2Ï€/n is used over n steps in each
of the cases n = 1000 Ã— 2k , k = 0, 1, . . . , 5. As a typical case, n = 2000 is shown in
the following diagrams, where the ï¬rst and second components are shown in the
left-hand diagram, and the third and fourth components on the right:
                              y2                                     y4



                                          y1                                    y3
1.4 The Euler method                                                                  17

   To assess the accuracy, in each of the six cases, it is convenient to calculate
yn âˆ’ y0 2 . For example, if n = 1000, then

                    yn = [1.015572, âˆ’0.358194, 0.319112, 0.907596]T ,
               yn âˆ’ y0 = [âˆ’0.015572, 0.358194, âˆ’0.319112, 0.092404]T ,
           yn âˆ’ y0 2 = 0.488791.

This single result gives only limited information on the accuracy to be expected from
the Euler method when carrying out this type of calculation. It will be more interesting
to use the sequence of six n values, n = 1000, 2000, . . . .32000, with corresponding
stepsizes h = 2Ï€/1000, 2Ï€/2000, . . . , 2Ï€/32000, displayed in a single diagram. As
we might expect, the additional work as n doubles repeatedly gives systematic
improvements. To illustrate the behaviour of this calculation for increasingly high
values of n, and increasingly low values of h, the following diagram is presented

                         error
                                10âˆ’0.5


                                   10âˆ’1
                                                            1

                                10âˆ’1.5
                                                     1


                                          10âˆ’3.5   10âˆ’3   10âˆ’2.5       10âˆ’2
                                                                   h


The triangle shown beside the main line suggests that the slope is close to 1.
   The slope of lines relating error to stepsize is of great importance since it predicts
the behaviour that could be expected for extremely small h. For example, if we needed
10âˆ’6 accuracy this ï¬gure suggests that we would need a stepsize of about 10âˆ’8 and
this would require a very large number of steps and therefore an unreasonable amount
of computer time. If, on the other hand, the slope were 2 or greater, we would obtain
much better performance for small h.


Experiments with diamond
In the case of the diamond problem, it is possible to evaluate the accumulated
error in a single orbit, evaluated using the Euler method. If n, the number of steps
to be evaluated, is a multiple of 4, there is zero error. We will consider the case
n = 4m + k, with m + k â‰¥ 4, where 1 â‰¤ k â‰¤ 3. Because the period is 4, the stepsize
is h = 4/n. In the ï¬rst quadrant, m + 1 steps moves the solution to the second
quadrant and a further m + 1 advances the solution to the interface with the third
quadrant. It then takes m + 2 steps to move to the fourth quadrant. This leaves
4m + k âˆ’ 2(m + 1) âˆ’ (m + 2) = m âˆ’ (4 âˆ’ k) steps to move within the fourth quadrant.
The ï¬nal position, relative to the initial point, is then
18                           1 Differential equations, numerical methods and algebraic analysis
                                               
m + 1 âˆ’1     m + 1 âˆ’1     m+2 1      m+kâˆ’4 1      4 kâˆ’4
           +            +          +            =         .
 n/4   1      n/4 âˆ’1      n/4 âˆ’1      n/4   1     n kâˆ’6

Computer simulations for this calculation can be misleading because of round-off
error.
Exercise 10 Find the error in calculating two orbits of diamond using n = 8m + k steps with
1 â‰¤ k â‰¤ 7, with m sufï¬ciently large.


An example of Taylor series

From the many choices available to test the Taylor series method, we will look at the
initial value problem
                           y = x2 + y2 ,     y(0) = 1.                       (1.4 e)
In [55] (Henrici, 1962), this problem was used to illustrate the disadvantages of
Taylor series methods, because of rapid growth of the complexity of the formulae
for y , y(3) , . . . . This was in the relatively early days of digital computing, and the
situation has now changed because of the feasibility of evaluating Taylor terms
automatically.
   But going back to hand calculations, the higher-derivatives do indeed blossom in
complexity, as we see from the ï¬rst few members of the sequence
                    y = x2 + y2 ,
                    y = 2x + 2x2 y + 2y3 ,
                  y(3) = 2 + 4xy + 2x4 + 8x2 y2 + 6y4 ,
                  y(4) = 4y + 12x3 + 20xy2 + 20x4 y + 40x2 y3 + 24y5 .
Recursive computation of derivatives
Although we will not discuss the systematic evaluation of higher derivatives for a
general problem, we can at least ï¬nd a simple recursion for the example problem
(1.4 e), based on the formula

                              y(n) = âˆ‚âˆ‚x y(nâˆ’1) + âˆ‚âˆ‚y y(nâˆ’1) f .

This gives the sequence of formulae
                 y(1) = x2 + (y(0) )2 ,
                 y(2) = 2x + 2y(0) y(1) ,
                 y(3) = 2 + 2y(0) y(2) + 2(y(1) )2 ,
                 y(4) = 2y(0) y(3) + 6y(1) y(2) ,
                 y(5) = 2y(0) y(4) + 8y(1) y(3) + 6(y(2) )2 ,
                 y(6) = 2y(0) y(5) + 10y(1) y(4) + 20y(2) y(3) ,
                 y(7) = 2y(0) y(6) + 12y(1) y(5) + 30y(2) y(4) + 20(y(3) )2 ,
1.5 Rungeâ€“Kutta methods                                                                       19

            y
          1.0



          0.8
                                                                          p=4
                                                                             p=3
          0.6

                                                                              p=2
          0.4
                                                                              p=1


          0.2



          0.0
            0.0           0.2            0.4         0.6            0.8         1.0   x

      Figure 1 Taylor series approximations of orders p = 1, 2, 3, 4 for y = x2 + y2 ,
      y(0.2) = 0.3



and the general result
                                 nâˆ’1          
                         y(n) = âˆ‘        nâˆ’1 (i) (nâˆ’1âˆ’i)
                                          i
                                             y y         ,        n â‰¥ 4.
                                 i=0

   To demonstrate how well the Taylor series works for this example problem,
Figure 1 is presented.



1.5 Rungeâ€“Kutta methods

One of the most widely used families of methods for approximating the solutions of
differential equations is the Rungeâ€“Kutta family. In one of these methods, a sequence
of n steps is taken from an initial point, x0 , to obtain an approximation to the solution
at x0 + nh, where h is the â€œstepsizeâ€.
    Each step has the same form and we will consider only the ï¬rst. Write the input
approximation as y0 â‰ˆ y(x0 ). The method involves ï¬rst obtaining approximations
Yi â‰ˆ y(x0 + hci ), i = 1, 2, . . . , s, where c1 , c2 , . . . , cs are the stage abscissae. Write
Fi = f (Yi ) for each stage so that Fi â‰ˆ y (x0 + hci ). The actual approximations used
for the stage values take the form

                         Yi = y0 + h âˆ‘ ai j Fj ,       i = 1, 2, . . . , s.               (1.5 a)
                                         j<i
20                           1 Differential equations, numerical methods and algebraic analysis

After the stage values, Y1 , Y2 , . . . , and the stage derivatives, F1 , F2 , . . . , have been
evaluated, the output to the step is found from
                                                    s
                                     y1 = y0 + h âˆ‘ bi Fi .                              (1.5 b)
                                                   i=1


Examples of explicit Rungeâ€“Kutta methods
The Runge second order methods
The method Runge-I is deï¬ned by the equations

                      Y1 = y0 ,                         F1 = f (x0 ,Y1 ),
                      Y2 = y0 + hF1 ,                   F2 = f (x0 + h,Y2 ),            (1.5 c)
                      y1 = y0 + 12 (hF1 + hF2 ).

Because F1 â‰ˆ y (x0 ) and F2 â‰ˆ y (x1 ), (1.5 c) can be seen as a generalization of the
trapezoidal rule:
                                                                  
                   y(x0 + h) âˆ’ y(x0 ) â‰ˆ 12 hy (x0 ) + hy (x0 + h) .

     The method Runge-II is deï¬ned by the equations

                         Y1 = y0 ,             F1 = f (x0 ,Y1 ),
                         Y2 = y0 + 12 hF1 ,    F2 = f (x0 + 12 h,Y2 ),                  (1.5 d)
                          y1 = y0 + hF2 .

Because F1 â‰ˆ y (x0 ) and F2 â‰ˆ y (x0 + 12 h), (1.5 c) can be seen as a generalization of
the midpoint rule:
                          y(x0 + h) âˆ’ y(x0 ) â‰ˆ hy (x0 + 12 h).

Third and fourth order methods
There are many possible methods with three stages and order three, and the following
is an example:
                 Y1 = y0 ,                   F1 = f (x0 ,Y1 ),
                 Y2 = y0 + 13 hF1 ,          F2 = f (x0 + 13 h,Y2 ),
                                                                               (1.5 e)
                 Y3 = y0 + 23 hF2 ,          F3 = f (x0 + 23 h,Y3 ),
                 y1 = y0 + 14 hF1 + 34 hF3 .
Similarly, the following four stage fourth order method is one of a large family:
                Y1 = y0 ,                                  F1 = f (x0 ,Y1 ),
                Y2 = y0 + 14 hF1 ,                         F2 = f (x0 + 14 h,Y2 ),
                Y3 = y0 + 12 hF2 ,                         F3 = f (x0 + 12 h,Y3 ),       (1.5 f)
                Y4 = y0 + hF1 âˆ’ 2hF2 + 2hF3 ,              F4 = f (x0 + h,Y4 ),
                 y1 = y0 + 16 hF1 + 23 hF3 + 16 hF4 .
1.5 Rungeâ€“Kutta methods                                                                         21

Naive veriï¬cation of order

Although the criteria for order of a Rungeâ€“Kutta method are quite sophisticated, it is
possible to demonstrate why (1.5 c) and (1.5 d) each has order 2, using very simple
arguments. We will assume that f is a sufï¬ciently smooth function so that we can
always use Taylor series in the form
                                                  n    i
                      y(x0 + h) = y(x0 ) + âˆ‘ hi! y(i) (x0 ) + O(hn+1 ).
                                                 i=1

Thus for the method (1.5 c), we can write for the truncation error of Y2 , as an
approximation to y(x0 + h),

         y(x0 + h) âˆ’Y2 = y(x0 + h) âˆ’ y(x0 ) âˆ’ hy (x0 ) = 12 h2 y (x0 ) + O(h3 ).

Assuming the existence and smoothness of fy , we can also write

                  y (x0 + h) âˆ’ F2 = f (x0 + h, y(x0 + h)) âˆ’ f (x0 + h,Y2 )
                                     = 12 h2 fy (x0 , y0 )y (x0 ) + O(h3 )

For the truncation error in y1 , as an approximation to y(x0 + h), we have

             y(x0 + h) âˆ’ y(x0 ) âˆ’ 12 hF1 âˆ’ 12 hF2
                                                                     
                = y(x0 + h) âˆ’ y(x0 ) âˆ’ 12 hy (x0 ) âˆ’ 12 hy (x0 + h)
                                                                
                                      + 12 h y (x0 + h) âˆ’ F2
                  
                = hy (x0 ) + 12 h2 y (x0 ) + 16 h3 y(3) (x0 )
                                                                                            
                          âˆ’ 12 hy (x0 ) âˆ’ 12 hy (x0 ) âˆ’ 12 h2 y (x0 ) âˆ’ 14 h3 y(3) (x0 )
                                                                          
                                        + 12 h 12 h2 fy (x0 , y0 )y (x0 ) + O(h4 )
                = âˆ’ 12 h y (x0 ) + 14 h3 fy (x0 , y0 )y (x0 ) + O(h4 ).
                    1 3 (3)



Exercise 11 Find a similar error formula for (1.5 d).


Exercise 12 Show that the method (1.5 e) has order 3.


Exercise 13 Show that the method (1.5 f) has order 4.



Representing methods with tableaux

It is customary to represent a particular Rungeâ€“Kutta method using only the coefï¬-
cients ai j , bi , ci appearing in (1.5 a, 1.5 b). These are, for the classical explicit case,
arranged in a tableau as follows
22                         1 Differential equations, numerical methods and algebraic analysis

                           0
                           c2       a21
                           c3       a31 a32
                            ..       ..  .. . .          .
                             .        .   .     .
                           cs       as1 as2 Â· Â· Â· as,sâˆ’1
                                    b1 b2 Â· Â· Â· bsâˆ’1 bs
For the methods we have already introduced, the corresponding tableaux are

                                    0
                                    1    1                                         method (1.5 c)
                                         1       1
                                         2       2

                                    0
                                    1    1
                                    2    2                                         method (1.5 d)
                                         0       1
                                    0
                                     1   1
                                     3   3                                         method (1.5 e)
                                     2           2
                                     3   0       3
                                         1               3
                                         4       0       4

                                     0
                                     1   1
                                     4   4
                                     1
                                     2   0 12                                      method (1.5 f)
                                     1   1 âˆ’2            2
                                         1               2   1
                                         6  0            3   6


Implicit Rungeâ€“Kutta methods

If the coefï¬cient matrix is full â€” that is, it contains non-zero elements on and above
the diagonal â€” the stages cannot be computed sequentially, and in order, using
explicit computations. Hence, an iteration scheme is normally required for their
evaluation. For example, the â€œtheta methodsâ€ with tableaux of the form

                                             Î¸       Î¸
                                                                                          (1.5 g)
                                                     1

are explicit only if Î¸ = 0. Two important special cases are Î¸ = 12 (the implicit
mid-point rule method) and Î¸ = 1 (the implicit Euler method). If f is sufï¬ciently
smooth and h is sufï¬ciently small, then the single stage Y is to be a solution of
Y = y0 + hÎ¸ f (Y ) and can be evaluated by functional iteration:

                     Y [0] = y0 ,
                                             
                     Y [k] = y0 + hÎ¸ f Y [kâˆ’1] ,                 k = 1, 2, . . .
1.5 Rungeâ€“Kutta methods                                                                23

For many problems, this iteration scheme is not efï¬cient, because of the severe
limitation that might need to be imposed on |h|, and some variant of Newton iteration
must be used.
   For s = 2, a well-known example of an implicit method is the so-called Radau
IIA method, with order 3, given by the tableau

                                              12 âˆ’ 12
                                          1    5   1
                                          3
                                               3    1                              (1.5 h)
                                          1    4    4
                                               3    1
                                               4    4

A second fully-implicit method with s = 2 is known as a Gauss method and has order
p = 4 [54] (Hammer, Hollingsworth,1955). The tableau is
                               âˆš                       âˆš
                         2âˆ’6 3                   4âˆ’6 3
                         1   1           1       1   1
                               âˆš         4
                                           âˆš
                         1   1
                         2+6 3
                                     1   1
                                     4+6 3
                                                    1
                                                    4
                                                          .                 (1.5 i)
                                              1               1
                                              2               2

This is one of a family of methods based on Gaussian quadrature and with order
p = 2s [65] (Kuntzmann, 1961), [9] (Butcher, 1964).



Inverse and adjoint methods


The stages and ï¬nal output for a generic Rungeâ€“Kutta method, assuming input value
y0 , are given by
                                      s
                      Yi = y0 + h âˆ‘ ai j f (Y j ),      i = 1, 2, . . . , s,       (1.5 j)
                                      j=1
                                       s
                     y1 = y0 + h âˆ‘ b j f (Y j ).                                   (1.5 k)
                                      j=1


If y1 is already known, y0 can be found by solving from (1.5 k), and the Yi can be
found by subtracting (1.5 k) from (1.5 j). This gives the method

                                 s
                  Yi = y1 + h âˆ‘ (ai j âˆ’ b j ) f (Y j ),     i = 1, 2, . . . , s,
                                j=1
                                 s
                  y0 = y1 + h âˆ‘ (âˆ’b j ) f (Y j ),
                                j=1


which exactly undoes the work of the original method. This leads to the deï¬nition
24                        1 Differential equations, numerical methods and algebraic analysis

 Deï¬nition 1.5A Given a tableau

                                          c      A
                                                 bT

 the inverse method (inverse tableau) is

                                c âˆ’ (bT 1)1          A âˆ’ 1bT
                                                      âˆ’bT


Closely related are â€œadjoint methodsâ€ in which the sign of h is changed in an inverse
method. For example, the adjoint method of (1.5 i) is

                          1 1
                               âˆš           1             1 1
                                                               âˆš
                          2+6 3                          4+6       3
                              âˆš            4
                                             âˆš
                          2âˆ’6 3          âˆ’
                          1 1          1   1               1
                                       4   6 3             4
                                           1               1
                                           2               2

which becomes identical with (1.5 i) if the stages are numbered in reverse order.
Methods with this property are â€œself adjointâ€ and have important properties computa-
tionally.



Methods with general index sets


The ï¬‚ow of an autonomous initial value problem on the interval [0, 1] can be written
as the solution to the integral equation

                                          Î¾              
               y(x0 + Î¾ h) = y0 + h           f y(x0 + Î·h) d Î·
                                      0
                                          1                       
                           = y0 + h           H(Î¾ âˆ’ Î·) f y(x0 + Î·h) d Î·,
                                      0

where                                â§
                                     âª
                                     â¨0, x < 0,
                                     âª
                               H(x) = 12 , x = 0,
                                     âª
                                     âª
                                     â©1, x > 0,

denotes the Heavyside function.
  This can be regarded as the continuous analogue of the s-stage Rungeâ€“Kutta
method with the coefï¬cient matrix given by
1.5 Rungeâ€“Kutta methods                                                                    25
                                                 â§
                                                 âª
                                                 âª                i < j,
                                                 â¨0,
                                              1
                                      ai j = 2s ,                 i = j,
                                            âª
                                            âª
                                            â©1,                   i > j.
                                              s

It is possible to place these two methods on a common basis by introducing an â€œindex
setâ€ I [14] (Butcher, 1972) which, in these examples, could be [0, 1] or {1, 2, 3, . . . , s}.
Adapting Rungeâ€“Kutta terminology slightly, the stage value function becomes a
bounded mapping I â†’ RN and the coefï¬cient matrix A becomes a bounded linear
operator on the space of bounded mappings I â†’ R to this same space. The ï¬nal
component of a Rungeâ€“Kutta method speciï¬cation, that is the row vector bT , becomes
a linear functional on the bounded mappings I â†’ R. More details will be presented
in Chapter 4.
    Even though energy-preserving Rungeâ€“Kutta methods, with ï¬nite I, do not exist,
the following method, the â€œAverage Vector Fieldâ€ method [80] (Quispel, McLaren,
2008) ) does satisfy this requirement [29] (Celledoni et al, 2009).

                                             1                   
                        y 1 = y0 + h             f (1 âˆ’ Î·)y0 + Î·y1 d Î·.
                                         0

For this method we have
                                         I = [0, 1],
                                                              1
                                 A(Î¾ )Ï† = Î¾                       Ï† (Î·) d Î·,
                                                          0
                                                         1
                                      bT Ï† =              Ï† (Î·) d Î·.
                                                     0

Methods based on the index set [0, 1] are referred to as â€œContinuous stage Rungeâ€“
Kutta methodsâ€.


Equivalence classes of Rungeâ€“Kutta methods


The two Rungeâ€“Kutta methods

                            0                                        0
                             1    1
                             2    2                  ,               1
                                                                     2
                                                                           1
                                                                           2
                            1    0      1
                                 0      1        0                         0   1

are equivalent in the sense that they give identical results because the third stage of
the method on the left is evaluated and never used. This is an example of Dahlquistâ€“
Jeltsch equivalence [39] (Dahlquist, Jeltsch, 2006). Similarly the two implicit meth-
ods
26                              1 Differential equations, numerical methods and algebraic analysis
            âˆš                               âˆš                    âˆš                            âˆš
      2âˆ’6 3                          4âˆ’6
      1 1                 1          1 1                     1 1             1          1 1
                                                 3           2+6 3                      4+6       3
          âˆš               4
                            âˆš                                    âˆš           4
                                                                               âˆš
      1 1
      2+6 3
                      1
                      4 + 1
                          6 3
                                       1
                                       4
                                                     ,       2âˆ’6 3
                                                             1 1         1
                                                                         4 âˆ’ 1
                                                                             6 3
                                                                                          1
                                                                                          4
                          1            1                                     1            1
                          2            2                                     2            2

are equivalent because they are the same method with their stages numbered in a
different order.
   Another example of an equivalent pair of methods, is

                 1
                 3
                            1
                            3
                                     1
                                    12   âˆ’ 12
                                           1
                                                         0
                 1
                         âˆ’ 12
                           1        1       1
                                                     âˆ’ 16
                                                                     1
                                                                     3
                                                                           5
                                                                           12   âˆ’ 12
                                                                                   1
                 3                  2       12
                 1          1        1      1
                                                     âˆ’ 14    ,       1     3
                                                                           4
                                                                                    1
                                                                                    4
                                                                                        .
                            2       4       2
                            1        1      1           1                  3        1
                 1          4       2       8           8                  4        4
                            3
                            8       8
                                     3      1
                                            3        âˆ’ 12
                                                       1


Suppose Y1âˆ— , Y2âˆ— are the solutions computed using the method on the right. Then
Y1 = Y2 = Y1âˆ— and Y3 = Y4 = Y2âˆ— satisfy the stage conditions for the method on the
left. Hence, the outputs for each of the methods are equal to the same result

                y1 = y0 + 38 h f (Y1 ) + 38 h f (Y2 ) + 13 h f (Y3 ) âˆ’ 12
                                                                        1
                                                                          h f (Y4 )
                     = y0 + 38 h f (Y1âˆ— ) + 38 h f (Y1âˆ— ) + 13 h f (Y2âˆ— ) âˆ’ 12
                                                                            1
                                                                               h f (Y2âˆ— )
                     = y0 + 34 h f (Y1âˆ— ) + 14 h f (Y2âˆ— ).

This is an example of Hundsdorferâ€“Spijker reducibility [58] (Hundsdorfer, Spijker,
1981).



Experiments with Rungeâ€“Kutta methods


The advantages of high order methods

As methods of higher and higher order are used, the cost also increases because the
number of f evaluations increases with the number of stages. But using a high order
method is usually an advantage over a low order method if sufï¬cient precision is
required.
   We will illustrate this in Figure 2, where a single half-orbit of the Kepler problem
with zero eccentricity is solved using four Rungeâ€“Kutta methods ranging from the
order 1 Euler method to the methods (1.5 c), (1.5 e) and (1.5 f). The orders of the
methods are attached to the plots of their error versus h behaviours on a log-log
scale. Also shown are triangles showing the exact slopes for comparison.
1.5 Rungeâ€“Kutta methods                                                                    27


                                                                  p=1

                   error
                                                          1                2
                                                                  p=
                                                  1
                        10âˆ’3


                                                          2                3
                                                                  p=
                        10âˆ’6                      1
                                                      1                    4
                                                                       =
                                              3                    p
                        10âˆ’9


                                                              4

                       10âˆ’12                              1

                                           10âˆ’3               10âˆ’2             h

    Figure 2 Error behaviour for Rungeâ€“Kutta methods with orders p = 1, 2, 3, 4, for the
    Kepler problem with zero eccentricity on the time interval [0, Ï€]




Methods for stiff problems

The aim in stiff methods is to avoid undue restriction on stepsize for stability reasons
but at the same time, to avoid excessive computational cost. In this brief introduction
we will compare two methods from the points of view of stepsize restriction, accuracy
and cost.
   The methods are the third order explicit method (1.5 e) and the implicit Radau
IIA method (1.5 h). In each case the problem (1.3 b) (p. 10) was solved with output
at x = 1 taking n steps with n ranging from 1 to 51200. The dependence of the
computational error on n, and therefore on h = 1/n is shown in the Figure 3, where
the method used in each result is attached to the curve. Note that the error in the
computation is only for a representative component y1 .
   From the ï¬gure we see that the output for the explicit method is useless unless
h < 0.02, approximately. This is a direct consequence of the stiffness of the problem.
But for the implicit Radau IIA method, there is no constraint on the stepsize except
that imposed by the need to obtain sufï¬cient accuracy. Because the computational
cost is much greater for the implicit method, many scientists and engineers are willing
to use explicit methods in spite of their unstable behaviour and the need to use small
stepsizes.
28                             1 Differential equations, numerical methods and algebraic analysis



            |error|
                10âˆ’3



                10âˆ’6                               t
                                               ici
                                             pl
                                          ex           ici
                                                          t
                                                  pl
                                              im
                10âˆ’9



                10âˆ’12


                              10âˆ’4         10âˆ’3               10âˆ’2   10âˆ’1          1
                                                                                       h

     Figure 3 Errors for the stiff problem (1.3 b), solved by an explicit and an implicit method



1.6 Multivalue methods

Linear multistep methods

Instead of calculating a number of stages in working from ynâˆ’1 to yn , a linear
multistep method makes use of past information evaluated in previous steps. That is,
yn is found from

          yn = a1 ynâˆ’1 + Â· Â· Â· + ak ynâˆ’k + hb1 f (ynâˆ’1 ) + Â· Â· Â· + hbk f (ynâˆ’k ).             (1.6 a)

In this terminology we will always assume that |ak | + |bk | > 0 because, if this were
not the case, k could be replaced by a lower positive integer. With this understanding,
we refer to this as a k-step method. The â€œexplicit caseâ€ (1.6 a) is generalized in (1.6 c)
below.
   In the k-step method (1.6 a), the quantities ai , bi , i = 1, 2, . . . , k, are numbers
chosen to obtain suitable numerical properties of the method. It is convenient to
introduce polynomials Ï, Ïƒ deï¬ned by

                               Ï(w) = wk âˆ’ a1 wkâˆ’1 âˆ’ Â· Â· Â· âˆ’ ak ,
                                                                                              (1.6 b)
                               Ïƒ (w) = b1 wkâˆ’1 + Â· Â· Â· + bk ,

so that the method can be referred to as (Ï, Ïƒ ) [36] (Dahlquist, 1956).
   The class of methods in this formulation can be extended slightly by adding a
term hb0 f (yn ) to the right-hand side of (1.6 a) or, equivalently, a term b0 wk to the
expression for Ïƒ (w). Computationally, this means that yn is deï¬ned implicitly as the
1.6 Multivalue methods                                                                       29

solution to the equation

     yn âˆ’ hb0 f (yn ) = a1 ynâˆ’1 + Â· Â· Â· + ak ynâˆ’k + hb1 f (ynâˆ’1 ) + Â· Â· Â· + hbk f (ynâˆ’k ).

In this case, (1.6 b) is replaced by

                           Ï(w) = wk âˆ’ a1 wkâˆ’1 âˆ’ Â· Â· Â· âˆ’ ak ,
                                                                                         (1.6 c)
                           Ïƒ (w) = b0 wk + b1 wkâˆ’1 + Â· Â· Â· + bk .

The most well-known examples of (1.6 b) are the Adamsâ€“Bashforth methods [3]
(Bashforth, Adams,1883), for which Ï(w) = wk âˆ’ wkâˆ’1 and the coefï¬cients in Ïƒ (w)
are chosen to obtain order p = k. Similarly, the well-known Adamsâ€“Moulton methods
[74] (Moulton, 1926) also have Ï(w) = wk âˆ’ wkâˆ’1 in (1.6 c), but the coefï¬cients in
Ïƒ (w) are chosen to obtain order p = k + 1.


Consistency, stability and convergence


 Deï¬nition 1.6A A method (Ï, Ïƒ ) is preconsistent if Ï(1) = 0. The method is
 consistent if it is preconsistent and also Ï  (1) = Ïƒ (1).

The signiï¬cance of Deï¬nition 1.6A is that for the problem y (x) = 0, y(0) = 1, if
ynâˆ’i = 1, i = 1, 2, . . . , k, then the value computed by the method in step number n is
also equal to the correct value yn = 1 if and only if âˆ‘ki=1 ai = 1, which is equivalent
to preconsistency. Furthermore, if the method is preconsistent and is used to solve
y (x) = 1, y(0) = 0, and the values ynâˆ’i = h(n âˆ’ i) then the result computed in step n
has the correct value yn = nh if and only if nh = âˆ‘ki=1 h(n âˆ’ i)ai + h âˆ‘ki=0 bi , which is
equivalent to the consistency condition, k âˆ’ âˆ‘ki=1 (k âˆ’ i)ai = âˆ‘ki=0 bi .

 Deï¬nition 1.6B A method (Ï, Ïƒ ) is stable if all solutions of the difference equa-
 tion
                        yn = a1 ynâˆ’1 + Â· Â· Â· + ak ynâˆ’k
 are bounded.



 Deï¬nition 1.6C A polynomial Ï satisï¬es the root condition if all zeros are in the
 closed unit disc and all multiple zeros are in the open unit disc.

The following result follows from the elementary theory of linear difference equa-
tions

 Theorem 1.6D A method (Ï, Ïƒ ) is stable if and only if Ï satisï¬es the root condi-
 tion.
30                            1 Differential equations, numerical methods and algebraic analysis

Exercise 14 Find the values of a1 and b1 for which the method (w2 âˆ’ a1 w + 12 , b1 w + 1) is
consistent. Is the resulting method stable?



Order of linear multistep methods
Dahlquist [36] (Dahlquist, 1956) has shown that

 Theorem 1.6E Given Ï(1) = 0, the pair (Ï, Ïƒ ) has order p if and only if

                                            Ï(1 + z)/z
                             Ïƒ (1 + z) =                + O(z p ),
                                            ln(1 + z)/z

 where ln denotes the principal value so that ln(1 + z)/z = 1 + O(z).

For convenience in applications of this result, note that
         1           1      1 2    1 3    19 4     3 5     863 6     275 7
     ln(1+z)/z = 1 + 2 z âˆ’ 12 z + 24 z âˆ’ 720 z + 160 z âˆ’ 60480 z + 24192 z
                       33953 8       8183 9     3663197 10
                   âˆ’ 3628800   z + 1036800 z + 43545600 z + O(z11 ).



Examples of linear multistep methods
The Euler method can be deï¬ned by Ï(w) = w âˆ’ 1, Ïƒ (w) = 1 and is the ï¬rst member
of the Adamsâ€“Bashforth family of methods [3] (Bashforth, Adams, 1883) The next
member is deï¬ned by

                          Ï(w) = w2 âˆ’ w,           Ïƒ (w) = 32 w âˆ’ 12 ,

because
                                         Ï(1+z)
                           Ïƒ (1 + z) =      z    (1 + 12 z) + O(z2 )
                                      = (1 + z)(1 + 12 z) + O(z2 )
                                      = 1 + 32 z
                                      = 32 w âˆ’ 12 ,     (w = 1 + z)

and has order 2 if correctly implemented. By this is meant the deï¬nition of y1 which
is required, in addition to y0 , to enable later values of the sequence of y values to be
computed. A simple choice is to deï¬ne y1 by a second order Rungeâ€“Kutta method,
such as (1.5 c) or (1.5 d).

Exercise 15 Show that the order 3 Adams-Bashforth method is deï¬ned by Ï(w) = w3 âˆ’ w2 ,
        12 w âˆ’ 3 w + 12 .
Ïƒ (w) = 23  2  4      5
1.6 Multivalue methods                                                                                    31

  Adamsâ€“Moulton methods [74] (Moulton, 1926) are found in a similar way to
Adamsâ€“Bashforth methods, except that Ïƒ (1 + z) is permitted to have a term in zk .
For k = 2 and k = 3, we have in turn Ï(w) = w âˆ’ 1 = z, Ï(w) = w2 âˆ’ w = (1 + z)z,
where we will always write w = 1 + z. The formulae for Ïƒ (w) are, respectively

   Ïƒ (w) = 1 + 12 z = 12 w + 12 ,                                                              (k = 2),
   Ïƒ (w) = (1 + z)(1 + 12 z âˆ’ 12
                              1 2                 5 2
                                 z ) = 1 + 32 z + 12     5 2
                                                     z = 12 w + 23 w âˆ’ 12
                                                                       1
                                                                          ,                    (k = 3).


Exercise 16 Show that the order 4 Adams-Moulton method is deï¬ned by Ï(w) = w3 âˆ’ w2 ,
                24 w âˆ’ 24 w + 24 .
Ïƒ (w) = 38 w3 + 19  2  5      1




General linear methods

Traditionally, practical numerical methods for differential equations are classiï¬ed
into Rungeâ€“Kutta methods and linear multistep methods.
      Combining these two families of methods into a single family gives methods
characterized by two complexity parameters r, the number of quantities passed from
step to step, and s, the number of stages. As for Rungeâ€“Kutta methods, the stages
will be denoted by Y1 , Y2 , . . . ,Ys and the corresponding stage derivatives by F1 , F2 ,
                                                                             [nâˆ’1] [nâˆ’1]
. . . , Fs . The r components of input to step number n will be denoted by y1 , y2 ,
         [nâˆ’1]                                      [n] [n]      [n]
. . . , yr , and the output from this step by y1 , y2 , . . . , yr . These quantities are
interrelated in terms of a partitioned (s + r) Ã— (s + r) matrix
                                                   
                                            A U
                                              B       V

using the equations
                       s            r
                                           [nâˆ’1]
              Yi = h âˆ‘ ai j Fj + âˆ‘ ui j y j        ,      Fi = f (Yi ),   i = 1, 2, . . . s,
                      j=1           j=1
                       s             r
              [n]                         [nâˆ’1]
             yi = h âˆ‘ bi j Fj + âˆ‘ vi j y j        ,                       i = 1, 2, . . . r.
                      j=1           j=1

The essential part of these relations can be written more compactly as

                              Y = h(A âŠ— I)F + (U âŠ— I)y[nâˆ’1] ,
                             y[n] = h(B âŠ— I)F + (V âŠ— I)y[nâˆ’1] ,

or, if no confusion is possible, as

                                     Y = hAF +Uy[nâˆ’1] ,
                                    y[n] = hBF +V y[nâˆ’1] .
32                          1 Differential equations, numerical methods and algebraic analysis

Consistency, stability and convergence

Generalizing the ideas of consistency to general linear methods is complicated
by the lack of a single obvious interpretation of the information passed between
steps of the method. However, we will try to aim for an interpretation in which
y[nâˆ’1] = uy(xnâˆ’1 ) + hvy (xnâˆ’1 ) + O(h2 ) for some u, v âˆˆ RN with the parameters
chosen so that at the completion of the step, y[n] = uy(xn ) + hvy (xn ) + O(h2 ), and
also so that the stage values satisfy Yi = y(xnâˆ’1 ) + O(h).
   We will explore the consequences of these assumptions by analysing the case
n = 1. We ï¬nd in turn
                                                              
                   1y(x0 ) = hAy (x0 ) +U uy(x0 ) + hvy (x0 ) + O(h),
                          U1 = 1,
                                                                
         u(y(x0 ) + hy (x0 ) = hB 1y (x0 ) +V uy(x0 ) + hvy (x0 ) + O(h2 ).
                      


For Rungeâ€“Kutta methods, there is only a single input and accordingly, r = 1. For
the method (1.5 f) the deï¬ning matrices are
                                   â¡                      â¤
                                       0 0 0 0 1
                                   â¢                      â¥
                               â¢ 1 0 0 0 1 â¥
                                   â¢   4                  â¥
                        A U        â¢                      â¥
                                = â¢ 0 12 0 0 1 â¥ .
                        B V        â¢                      â¥
                                   â¢ 1 âˆ’2 2 0 1 â¥
                                   â£                      â¦
                                       1      2    1
                                       6    0 3    6   1

By contrast, for a linear multistep method, s = 1. In the case of the order 3 Adamsâ€“
Bashforth method, the deï¬ning matrices are
                                    â¡                         â¤
                                       0 1 23       âˆ’ 4     5
                                    â¢           12    3    12 â¥
                                â¢ â¢  0   1    23
                                                    âˆ’ 4     5 â¥
                                                           12 â¥
                        A U         â¢           12    3
                                                              â¥
                                  =â¢â¢  1 0 0 0 0 â¥            â¥.
                        B V         â¢                         â¥
                                    â¢ 0 0 1 0 0 â¥
                                    â£                         â¦
                                          0    0    0     1     0

   Moving away from traditional methods consider the method with r = 2, s = 3,
with matrices                    â¡                    â¤
                                   0 0 0 1 0
                                 â¢                    â¥
                            â¢     1
                                 â¢ 2 0 0 1 1 â¥
                                                      â¥
                     A U         â¢                    â¥
                              =â¢ â¢ 0 1 0 1 0 â¥        â¥.               (1.6 d)
                     B V         â¢ 1 2 1              â¥
                                 â¢                    â¥
                                 â£ 6 3 6 1 0 â¦
                                    4 âˆ’4
                                    1   3   1
                                            2   0 0
1.7 B-series analysis of numerical methods                                               33

For a person acquainted only with traditional Rungeâ€“Kutta and linear multistep
methods, (1.6 d) might seem surprising. However, it is for the analysis of methods
like this that the theory of B-series has a natural role. In particular, we note that if the
                                                 [n]
method is started in a suitable manner, then y1 â‰ˆ y(xn ) to a similar accuracy as for
the fourth order Rungeâ€“Kutta method. One possible starting scheme is based on the
tableau
                                        0
                                             1     1
                                 Rh =        2
                                             1
                                                   2
                                                        1
                                                                  .
                                             2     0    2
                                                 âˆ’ 14   1
                                                        8
                                                              1
                                                              8


Starting with the initial value y0 , the initial y[0] can be computed by
                                       [0]
                                      y1 = y0 ,
                                                                                     (1.6 e)
                                       [0]
                                      y2 = R h y0 âˆ’ y0 .

   In Chapter 6, Section 6.4 (p. 225), the method (1.6 d), together with (1.6 e) as
starting method, will be used as an illustrative example.



1.7 B-series analysis of numerical methods

Higher derivative methods

The Euler method was introduced in Section 1.4 (p. 14) as the ï¬rst order case of the
Taylor series method. The more sophisticated methods are attempts to improve this
basic approximation method.
   The practical advantage of methods which require the evaluation of higher deriva-
tives hinges on the relative cost of these evaluations compared with the cost of just
the ï¬rst derivative. But there are other reasons for obtaining formulae for higher
derivatives in a systematic way; these are that this information is required for the
analysis of so-called B-series.
   For a given autonomous problem,
                        
          y (x) = f y(x) , y(x0 ) = y0 ,      y : R â†’ R N , f : R N â†’ RN ,

written in component by component form

                      d yi
                      d x = f (y , y , . . . , y ),         i = 1, 2, . . . , N,
                             i 1 2              N


we will ï¬nd a formula for the second derivative of yi . This can be obtained by
the chain-rule followed by a substitution of the known ï¬rst derivative of a generic
34                               1 Differential equations, numerical methods and algebraic analysis

component f j . That is,
                                                      N
                                          d2 yi            âˆ‚ f i dyj
                                                =    âˆ‘ âˆ‚yj dx
                                          d x2
                                                     j=1
                                                      N
                                                           âˆ‚ fi
                                                 = âˆ‘ âˆ‚ y j f j.
                                                     j=1

This can be written in a more compact form by using subscripts to indicate partial y
derivatives. That is, f ji := âˆ‚ f i /âˆ‚ y j . A further simpliï¬cation results by adopting the
â€œsummation conventionâ€, in which repeated sufï¬xes in expressions like f ji f j imply
summation, without this being written explicitly. Hence, we can write

                                               d2 yi
                                                     = f ji f j .
                                               d x2

Take this further and ï¬nd formulae for the third and fourth derivatives
               d 3 yi
                             f f + f ji fkj f k ,
                          i j k
                      = f jk
               d x3
               d 4 yi     i
                      = f jk f j f k f  + 3 f jk
                                                i j k 
                                                   f f f + f ji fkj f k f  + f ji fkj fk f  .
               d x4

From the sequence of derivatives, evaluated at y0 , the Taylor series can be evaluated.
   In further developments, we will avoid the use of partial derivatives, in favour of
                                                                i , . . . , we will use the total
FreÌchet derivatives. That is, in place of the tensors f ji , f jk
                
derivatives f , f , . . . . Evaluated at y0 , these will be denoted by

                                                f = f (y0 ),
                                               f  = f  (y0 ),
                                               f  = f  (y0 ),
                                                  ..     ..
                                                   .      .


Formal Taylor series

The ï¬rst few terms of the formal Taylor series for the solution at x = x0 + h are

                y(x0 + h) = y0 + hf + 12 h2 f  f + 16 h3 f  ff + 16 h3 f  f  f + Â· Â· Â·          (1.7 a)


Application to the theta method

The result computed by the theta method (1.5 g) (p. 22) has a Taylor expansion, with
a resemblance to (1.7 a). That is,

                  y1 = y0 + hf + Î¸ h2 f  f + 12 Î¸ 2 h3 f  ff + Î¸ 2 h3 f  f  f + Â· Â· Â·           (1.7 b)
1.7 B-series analysis of numerical methods                                             35

A comparison of (1.7 a) and (1.7 b) suggests that the error in approximating the exact
solution by the theta method is O(h2 ) for Î¸ = 12 and O(h3 ) for Î¸ = 12 . Useful though
this observation might be, it is just the start of the story. We want to be able to carry
out straight-forward analyses of methods using this type of â€œB-seriesâ€ expansion.
We want to be able to do manipulations of B-series as symbolic counterparts to the
computational equations deï¬ning the result, and the steps leading to this result, in a
wide range of numerical methods.

Elementary differentials and trees

The expressions f, f  f, f  ff and f  f  f are examples of â€œelementary differentialsâ€
and, symbolically, they have a graph-theoretical analogue. Corresponding to f is an
individual in a genealogical tree; corresponding to f  is an individual with a link to a
possible child. The term f  f corresponds to this link having been made to the child
represented by f. The bi-linear operator f  corresponds to an individual with two
possible links and in f  ff these links are ï¬lled with copies of the child represented
by f.
   Finally, in these preliminary remarks, f  f  f corresponds to a three generation
family with the ï¬rst f  playing the role of grandparent, the second f  playing the role
of a parent, and the child of the grandparent; and the ï¬nal operand f playing the role
of grandchild and child, respectively, of the preceding f  operators.
   The relationship between elementary differentials and trees can be illustrated in
diagrams.
                                                                  f
                                      f              f                f    f
                          f           f                       f         f
We can extend these ideas to trees and elementary differentials of arbitrary complexity,
as shown in the diagram

                                                                      f
                                       f f f      f                   f
                                       f f f 
                                                                      f
                                                     f (4)
   The elementary differential corresponding to this diagram can be written in
a variety of ways. For instance one can insert spaces to emphasize the separation
between the four operands of f (4) , or use power notation to indicate repeated operands
and operators:
                                  f (4) f  ff  ff  fff  f  f
                                = f (4) f  f f  f f  ff f  f  f
                                = f (4) (f  f)2 f  f 2 f  f  f
                                = f (4) (f  f)2 f  f 2 f  f  f
36                                        1 Differential equations, numerical methods and algebraic analysis

As further examples, we show the trees with four vertices, together with the corre-
sponding elementary differentials:




                              f (3) f 3        f  ff  f       f  f  f 2       f f f f


Exercise 17 Find the trees corresponding to each of the elementary differentials:
(a) f  (f  f)2 , (b) f (4) f 3 f  f, (c) f  f  f 2 f  f.

Exercise 18 Find the elementary differentials corresponding to each of the trees:
(a)   , (b)       , (c)   .




Summary of Chapter 1 and the way forward
Summary

Although this book is focussed on the algebraic analysis of numerical methods, a
good background in both ordinary differential equations and numerical methods for
their solution is essential.
   In this chapter a very basic survey of these important topics has been presented.
That is, the fundamental theory of initial value problems is discussed, partly through a
range of test problems. These problems arise from standard physical modelling, with
the addition of a number of contrived and artiï¬cial problems. This is then followed
by a brief look at the classical one-step and linear multistep methods, and an even
briefer look at some all encompassing multivalue-multistage methods (â€œgeneral
linear methodsâ€). Some of the methods are accompanied by numerical examples,
underlining some of their properties.
   As a preview for later chapters, B-series are brieï¬‚y introduced, along with trees
and elementary differentials.


The way forward

The current chapter includes preliminary notes on some of the later chapters. This is
indicated in the following diagram by a dotted line pointing to these speciï¬c chapters.
A full line pointing between chapters indicates a stronger prerequisite.

                                                                                5

              1                  2             3             4                                   7

                                                                                6
1.7 B-series analysis of numerical methods                                                      37

Teaching and study notes

It is a good idea to supplement the reading of this chapter using some of the many
books available on this subject. Those best known to the present author are
Ascher, U.M. and Petzold, L.R. Computer Methods for Ordinary Differential Equa-
tions and Differential-Algebraic Equations (1998) [1]
Butcher, J.C. Numerical Methods for Ordinary Differential Equations (2016) [20]
Gear, C.W. The Numerical Integration of Ordinary Differential Equations (1967)
[44]
Hairer, E., NÃ¸rsett, S.P. and Wanner, G. Solving Ordinary Differential Equations
I: Nonstiff Problems (1993) [50]
Hairer, E. and Wanner G. Solving Ordinary Differential Equations II: Stiff and
Differential-Algebraic Problems (1996) [53]
Henrici, P. Discrete Variable Methods in Ordinary Differential Equations (1962)
[55]
Iserles, A. A First Course in the Numerical Analysis of Differential Equations (2008)
[61]
Lambert, J.D. Numerical Methods for Ordinary Differential Systems (1991) [67]

Projects
Project 1 Explore existence and uniqueness questions for problems satisfying a local Lipschitz
condition.
Project 2 Find numerical solutions, using a variety of methods, for the simple pendulum. Some
questions to ask are (i) does the quality of the approximations deteriorate with increased initial
energy? and (ii) how well preserved is the Hamiltonian.?
Project 3   Learn all you can about fourth order Rungeâ€“Kutta methods.
Project 4   Read about predictor-corrector methods in [67] or some other text-book.
